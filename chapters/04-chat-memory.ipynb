{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2XLIuRtf-Ct"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dasarpai/langchain-course/blob/main/chapters/04-chat-memory.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective\n",
        "1. When we are creating chatbots we need memory to remember old conversation.\n",
        "2. There are different ways of treating those history messages: keep entire, keep k, keep summary.\n",
        "3. How to tackle the sitution with different sessions\n",
        "\n",
        "This exercise is to understand that.\n",
        "\n",
        "I have run this entire notebook and all is working smooth on colab."
      ],
      "metadata": {
        "id": "HhcUHGr7mYqC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEZdSobItaTI"
      },
      "source": [
        "### LangChain Essentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaGqZi42taTJ"
      },
      "source": [
        "# Conversational Memory for OpenAI - LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkg11xv9taTK"
      },
      "source": [
        "Conversational memory allows our chatbots and agents to remember previous interactions within a conversation. Without conversational memory, our chatbots would only ever be able to respond to the last message they received, essentially forgetting all previous messages with each new message.\n",
        "\n",
        "Naturally, conversations require our chatbots to be able to respond over multiple interactions and refer to previous messages to understand the context of the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ydYy0Wc7tfLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3abdab-f80c-4fd0-84be-f70625f92996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m695.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.3/333.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core==0.3.33 \\\n",
        "  langchain-openai==0.3.3 \\\n",
        "  langchain-community==0.3.16 \\\n",
        "  langsmith==0.3.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NIJ-etvtaTK"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euKN4BNCtaTK"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "IrH6arEXgHrV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FSfe1JIwtaTK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-chat-memory-openai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ57c5BxtaTL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAYjdtcPtaTL"
      },
      "source": [
        "## LangChain's Memory Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czt55jO1taTL"
      },
      "source": [
        "LangChain versions `0.0.x` consisted of various conversational memory types. Most of these are due for deprecation but still hold value in understanding the different approaches that we can take to building conversational memory.\n",
        "\n",
        "Throughout the notebook we will be referring to these _older_ memory types and then rewriting them using the recommended `RunnableWithMessageHistory` class. We will learn about:\n",
        "\n",
        "* `ConversationBufferMemory`: the simplest and most intuitive form of conversational memory, keeping track of a conversation without any additional bells and whistles.\n",
        "* `ConversationBufferWindowMemory`: similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages.\n",
        "* `ConversationSummaryMemory`: rather than keeping track of the entire conversation, this memory type keeps track of a summary of the conversation.\n",
        "* `ConversationSummaryBufferMemory`: merges the `ConversationSummaryMemory` and `ConversationTokenBufferMemory` types.\n",
        "\n",
        "We'll work through each of these memory types in turn, and rewrite each one using the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brmml7G-taTL"
      },
      "source": [
        "## Initialize our LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1vhQcsxtaTL"
      },
      "source": [
        "Before jumping into our memory types, let's initialize our LLM. We will use OpenAI's `gpt-4o-mini` model, if you need an API key you can get one from [OpenAI's website](https://platform.openai.com/settings/organization/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bNMunYQZtaTL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \\\n",
        "    getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "# For normal accurate responses\n",
        "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsRVD0HctaTM"
      },
      "source": [
        "## 1. `ConversationBufferMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40zJzXXXtaTM"
      },
      "source": [
        "`ConversationBufferMemory` is the simplest form of conversational memory, it is _literally_ just a place that we store messages, and then use to feed messages into our LLM.\n",
        "\n",
        "Let's start with LangChain's original `ConversationBufferMemory` object, we are setting `return_messages=True` to return the messages as a list of `ChatMessage` objects — unless using a non-chat model we would always set this to `True` as without it the messages are passed as a direct string which can lead to unexpected behavior from chat LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY-BxLW3taTM",
        "outputId": "ebba573f-9d6b-4593-c1c7-4f0ea59540f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-02dca0ac940b>:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMkCBXxitaTM"
      },
      "source": [
        "There are several ways that we can add messages to our memory, using the `save_context` method we can add a user query (via the `input` key) and the AI's response (via the `output` key). So, to create the following conversation:\n",
        "\n",
        "```\n",
        "User: Hi, my name is James\n",
        "AI: Hey James, what's up? I'm an AI model called Zeta.\n",
        "User: I'm researching the different types of conversational memory.\n",
        "AI: That's interesting, what are some examples?\n",
        "User: I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
        "AI: That's interesting, what's the difference?\n",
        "User: Buffer memory just stores the entire conversation, right?\n",
        "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
        "User: Buffer window memory stores the last k messages, dropping the rest.\n",
        "AI: Very cool!\n",
        "```\n",
        "\n",
        "We do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mUPUE9DhtaTM"
      },
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Hi, my name is James\"},  # user message\n",
        "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
        "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
        "    {\"output\": \"Very cool!\"}  # AI response\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw6C4W54taTM"
      },
      "source": [
        "Before using the memory, we need to load in any variables for that memory type — in this case, there are none, so we just pass an empty dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRLwtZvWtaTM",
        "outputId": "2593d442-db9e-46c3-8c79-206c26dd98ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_qjbVustaTM"
      },
      "source": [
        "With that, we've created our buffer memory. Before feeding it into our LLM let's quickly view the alternative method for adding messages to our memory. With this other method, we pass individual user and AI messages via the `add_user_message` and `add_ai_message` methods. To reproduce what we did above, we do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOjy4g_GtaTM",
        "outputId": "ad0d320b-a1e4-472f-e990-c57013bd2070"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
        "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1tkc6DOtaTM"
      },
      "source": [
        "The outcome is exactly the same in either case. To pass this onto our LLM, we need to create a `ConversationChain` object — which is already deprecated in favor of the `RunnableWithMessageHistory` class, which we will cover in a moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwgjzC0ktaTM",
        "outputId": "3a7bb243-e85a-4791-f3ee-298e593e97e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-af603a1bd914>:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  chain = ConversationChain(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx1fMZLWtaTN",
        "outputId": "8375d1ff-4565-42cf-f3ed-5a96891827d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Your name is James!', additional_kwargs={}, response_metadata={})],\n",
              " 'response': 'Your name is James!'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQTzPaovtaTN"
      },
      "source": [
        "### `ConversationBufferMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLZa82dhtaTN"
      },
      "source": [
        "As mentioned, the `ConversationBufferMemory` type is due for deprecation. Instead, we can use the `RunnableWithMessageHistory` class to implement the same functionality.\n",
        "\n",
        "When implementing `RunnableWithMessageHistory` we will use **L**ang**C**hain **E**xpression **L**anguage (LCEL) and for this we need to define our prompt template and LLM components. Our `llm` has already been defined, so now we just define a `ChatPromptTemplate` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UijZzCtutaTN"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    ChatPromptTemplate\n",
        ")\n",
        "\n",
        "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnEAavfntaTN"
      },
      "source": [
        "We can link our `prompt_template` and our `llm` together to create a pipeline via LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ywxify04taTN"
      },
      "outputs": [],
      "source": [
        "pipeline = prompt_template | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jhOF-uVtaTN"
      },
      "source": [
        "Our `RunnableWithMessageHistory` requires our `pipeline` to be wrapped in a `RunnableWithMessageHistory` object. This object requires a few input parameters. One of those is `get_session_history`, which requires a function that returns a `ChatMessageHistory` object based on a session ID. We define this function ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BXKmcliGtaTN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "\n",
        "chat_map = {}\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39LBschEtaTN"
      },
      "source": [
        "We also need to tell our runnable which variable name to use for the chat history (ie `history`) and which to use for the user's query (ie `query`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zHQRgNfrtaTO"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wwEeNAVtaTO"
      },
      "source": [
        "Now we invoke our runnable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ESDoue1taTO",
        "outputId": "fed138a3-3a05-401f-ec49-f15dfe66ac5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 26, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-c6ec7bef-4cf5-46e7-81b1-54fe3c94afa8-0', usage_metadata={'input_tokens': 26, 'output_tokens': 11, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avL-i76_taTO"
      },
      "source": [
        "Our chat history will now be memorized and retrieved whenever we invoke our runnable with the same session ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgfzWIBstaTO",
        "outputId": "44bdc4b4-cb12-48b3-e0e0-1db96ad0e312"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 50, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-2a190470-a9ff-4bd4-994c-582cc7149e63-0', usage_metadata={'input_tokens': 50, 'output_tokens': 13, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"id_123\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmjSuam-taTO"
      },
      "source": [
        "We have now recreated the `ConversationBufferMemory` type using the `RunnableWithMessageHistory` class. Let's continue onto other memory types and see how these can be implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0MliXn-taTO"
      },
      "source": [
        "## 2. `ConversationBufferWindowMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lziswUkqtaTO"
      },
      "source": [
        "The `ConversationBufferWindowMemory` type is similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages. There are a few reasons why we would want to keep only the last `k` messages:\n",
        "\n",
        "* More messages mean more tokens are sent with each request, more tokens increases latency _and_ cost.\n",
        "\n",
        "* LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or _\"forget\"_ information provided to them. Conciseness is key to high performing LLMs.\n",
        "\n",
        "* If we keep _all_ messages we will eventually hit the LLM's context window limit, by adding a window size `k` we can ensure we never hit this limit.\n",
        "\n",
        "The buffer window solves many problems that we encounter with the standard buffer memory, while still being a very simple and intuitive form of conversational memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBb46qDDtaTO",
        "outputId": "87ccf1df-b542-4c21-e7f7-f99491b5c3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-c190440c16c0>:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to6rLd_ataTO"
      },
      "source": [
        "We populate this memory using the same methods as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pfxuLS7taTP",
        "outputId": "0c94118b-76a4-4b85-bf6c-eabd649dd840"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
        "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV_6vOOwtaTP"
      },
      "source": [
        "As before, we use the `ConversationChain` object (again, this is deprecated and we will rewrite it with `RunnableWithMessageHistory` in a moment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "opmjk94ltaTP"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ-g586vtaTP"
      },
      "source": [
        "Now let's see if our LLM remembers our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWa5mhzftaTP",
        "outputId": "40338ef1-1e84-45da-84d1-833cdae22518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"I'm sorry, but I don't have access to your name or any personal information. If you'd like to share it, I can remember it for the duration of our conversation!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GrFsMbStaTP"
      },
      "source": [
        "The reason our LLM can no longer remember our name is because we have set the `k` parameter to `4`, meaning that only the last messages are stored in memory, as we can see above this does not include the first message where we introduced ourselves.\n",
        "\n",
        "Based on the agent forgetting our name, we might wonder _why_ we would ever use this memory type compared to the standard buffer memory. Well, as with most things in AI, it is always a trade-off. Here we are able to support much longer conversations, use less tokens, and improve latency — but these come at the cost of forgetting non-recent messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alyXf5nKtaTP"
      },
      "source": [
        "### `ConversationBufferWindowMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxaUDp7GtaTP"
      },
      "source": [
        "To implement this memory type using the `RunnableWithMessageHistory` class, we can use the same approach as before. We define our `prompt_template` and `llm` as before, and then wrap our pipeline in a `RunnableWithMessageHistory` object.\n",
        "\n",
        "For the window feature, we need to define a custom version of the `InMemoryChatMessageHistory` class that removes any messages beyond the last `k` messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fQUv8yPltaTP"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, k: int):\n",
        "        super().__init__(k=k)\n",
        "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages.\n",
        "        \"\"\"\n",
        "        self.messages.extend(messages)\n",
        "        self.messages = self.messages[-self.k:]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uoDJ38vztaTP"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
        "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
        "    # remove anything beyond the last\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GsXhm811taTP"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQR8TZLGtaTQ"
      },
      "source": [
        "Now we invoke our runnable, this time passing a `k` parameter via the `config` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBCnrghLtaTQ",
        "outputId": "fb8cc8e0-349c-473f-81bb-f761e6a6200d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history called with session_id=id_k4 and k=4\n",
            "Initializing BufferWindowMessageHistory with k=4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 26, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-79a4bfdd-91e3-4e6b-a0e7-6beb1af75276-0', usage_metadata={'input_tokens': 26, 'output_tokens': 11, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AvqsaA3taTQ"
      },
      "source": [
        "We can also modify the messages that are stored in memory by modifying the records inside the `chat_map` dictionary directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDMrCS26taTQ",
        "outputId": "b443dd13-6014-4480-a630-ddc6ae3d61a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "chat_map[\"id_k4\"].clear()  # clear the history\n",
        "\n",
        "# manually insert history\n",
        "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k4\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bzArAVDtaTQ"
      },
      "source": [
        "Now let's see at which `k` value our LLM remembers our name — from the above we can already see that with `k=4` our name is not mentioned, so when running with `k=4` we should expect the LLM to forget our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yn3K6fxtaTQ",
        "outputId": "8f51f426-8d30-4fd1-ea31-7275cb57a20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history called with session_id=id_k4 and k=4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but I don't have access to personal information about users unless you've shared it in this conversation. If you'd like me to remember your name, please tell me!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 79, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-b955e8be-4a8f-496b-97bc-ac52e53abce2-0', usage_metadata={'input_tokens': 79, 'output_tokens': 35, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN7am67YtaTQ"
      },
      "source": [
        "Now let's initialize a new session with `k=14`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOXU6nI0taTQ",
        "outputId": "0dd9c7b9-6e53-40b1-ff5c-48327bdd90ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history called with session_id=id_k14 and k=14\n",
            "Initializing BufferWindowMessageHistory with k=14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 26, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-4d46e494-a8c5-4153-adc8-91488bf385f9-0', usage_metadata={'input_tokens': 26, 'output_tokens': 11, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPyLeEOtaTQ"
      },
      "source": [
        "We'll manually insert the remaining messages as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuyHMnkCtaTQ",
        "outputId": "7c2caf6e-8566-4df6-8b90-24d198ec0948"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 26, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-4d46e494-a8c5-4153-adc8-91488bf385f9-0', usage_metadata={'input_tokens': 26, 'output_tokens': 11, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k14\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RVgWS6EtaTQ"
      },
      "source": [
        "Now let's see if the LLM remembers our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8X1xV9ItaTQ",
        "outputId": "6b6d5d72-bb38-41af-a8f7-edfe28f29d8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history called with session_id=id_k14 and k=14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 175, 'total_tokens': 181, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-b4ae2c57-4ae3-4683-bc71-60f20dee3189-0', usage_metadata={'input_tokens': 175, 'output_tokens': 6, 'total_tokens': 181, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3SqKjEvtaTQ",
        "outputId": "50912dcf-4475-4b46-f4a0-3c77de01600b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 26, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-4d46e494-a8c5-4153-adc8-91488bf385f9-0', usage_metadata={'input_tokens': 26, 'output_tokens': 11, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Your name is James.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 156, 'total_tokens': 162, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-5469cb00-7252-40ee-a0de-1fbd80494f3d-0', usage_metadata={'input_tokens': 156, 'output_tokens': 6, 'total_tokens': 162, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Your name is James.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 175, 'total_tokens': 181, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-b4ae2c57-4ae3-4683-bc71-60f20dee3189-0', usage_metadata={'input_tokens': 175, 'output_tokens': 6, 'total_tokens': 181, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "chat_map[\"id_k14\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R91uVCqFtaTQ"
      },
      "source": [
        "That's it! We've rewritten our buffer window memory using the recommended `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK51EzantaTR"
      },
      "source": [
        "## 3. `ConversationSummaryMemory`\n",
        "\n",
        "Next up we have `ConversationSummaryMemory`, this memory type keeps track of a summary of the conversation rather than the entire conversation. This is useful for long conversations where we don't need to keep track of the entire conversation, but we do want to keep some thread of the full conversation.\n",
        "\n",
        "As before, we'll start with the original memory class before reimplementing it with the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1DRv1umtaTR",
        "outputId": "9a1cf1dd-a59e-477e-977a-8127ebce1037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-384808bb9010>:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=llm)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDK1xTiQtaTR"
      },
      "source": [
        "Unlike with the previous memory types, we need to provide an `llm` to initialize `ConversationSummaryMemory`. The reason for this is that we need an LLM to generate the conversation summaries.\n",
        "\n",
        "Beyond this small tweak, using `ConversationSummaryMemory` is the same as with our previous memory types when using the deprecated `ConversationChain` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "A5hxfssAtaTR"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5nHb-SqtaTR"
      },
      "source": [
        "Let's test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwCtQA0RtaTR",
        "outputId": "b8daa141-9c58-4fc3-fbaa-0ffaf8647379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: hello there my name is James\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James. The AI greets James and expresses its willingness to chat and help, inquiring about how his day is going.\n",
            "Human: I am researching the different types of conversational memory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James, and the AI greets him, asking about his day. James mentions he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory includes short-term memory for single conversations, long-term memory for retaining information across interactions, contextual memory for maintaining coherence, and dynamic memory for adapting to new information. The AI then inquires if James is focusing on a specific type of conversational memory or exploring the topic more broadly.\n",
            "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James, and the AI greets him, asking about his day. James mentions he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory includes short-term memory for single conversations, long-term memory for retaining information across interactions, contextual memory for maintaining coherence, and dynamic memory for adapting to new information. The AI then inquires if James is focusing on a specific type of conversational memory or exploring the topic more broadly. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI explains that ConversationBufferMemory tracks recent exchanges to maintain context, while ConversationBufferWindowMemory uses a sliding window to dynamically adjust the number of retained messages for efficient memory management. The AI then asks if James is interested in how these memory types impact conversation quality or their technical implementations.\n",
            "Human: Buffer memory just stores the entire conversation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James, and the AI greets him, asking about his day. James mentions he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory includes short-term memory for single conversations, long-term memory for retaining information across interactions, contextual memory for maintaining coherence, and dynamic memory for adapting to new information. The AI then inquires if James is focusing on a specific type of conversational memory or exploring the topic more broadly. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI explains that ConversationBufferMemory tracks recent exchanges to maintain context, while ConversationBufferWindowMemory uses a sliding window to dynamically adjust the number of retained messages for efficient memory management. The AI then asks if James is interested in how these memory types impact conversation quality or their technical implementations. James notes that buffer memory just stores the entire conversation. The AI agrees, explaining that ConversationBufferMemory stores the entire conversation history for context but can lead to challenges like increased memory usage. In contrast, ConversationBufferWindowMemory retains only a certain number of recent messages to optimize memory usage while still providing context. The AI then asks if James is considering the trade-offs between these two types of memory in his research or looking into their effects on user experience.\n",
            "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
              " 'history': 'The human introduces himself as James, and the AI greets him, asking about his day. James mentions he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory includes short-term memory for single conversations, long-term memory for retaining information across interactions, contextual memory for maintaining coherence, and dynamic memory for adapting to new information. The AI then inquires if James is focusing on a specific type of conversational memory or exploring the topic more broadly. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI explains that ConversationBufferMemory tracks recent exchanges to maintain context, while ConversationBufferWindowMemory uses a sliding window to dynamically adjust the number of retained messages for efficient memory management. The AI then asks if James is interested in how these memory types impact conversation quality or their technical implementations. James notes that buffer memory just stores the entire conversation. The AI agrees, explaining that ConversationBufferMemory stores the entire conversation history for context but can lead to challenges like increased memory usage. In contrast, ConversationBufferWindowMemory retains only a certain number of recent messages to optimize memory usage while still providing context. The AI then asks if James is considering the trade-offs between these two types of memory in his research or looking into their effects on user experience.',\n",
              " 'response': \"That's right! ConversationBufferWindowMemory indeed retains only the last k messages, which helps manage memory more efficiently by discarding older messages that may no longer be relevant. This approach can strike a balance between maintaining context and minimizing memory usage. It allows for a more dynamic interaction, as the AI can adapt to recent exchanges without being bogged down by an extensive history. Are you exploring how this trade-off affects user experience or perhaps looking into specific applications where one might be preferred over the other?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"hello there my name is James\"})\n",
        "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
        "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
        "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
        "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfgmdXd2taTR"
      },
      "source": [
        "We can see how the conversation summary varies with each new message. Let's see if the LLM is able to recall our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHnh4Y1OtaTR",
        "outputId": "50079ffd-7226-4457-fb23-58d6c6d68de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James, and the AI greets him, asking about his day. James mentions he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory includes short-term memory for single conversations, long-term memory for retaining information across interactions, contextual memory for maintaining coherence, and dynamic memory for adapting to new information. The AI then inquires if James is focusing on a specific type of conversational memory or exploring the topic more broadly. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI explains that ConversationBufferMemory tracks recent exchanges to maintain context, while ConversationBufferWindowMemory uses a sliding window to dynamically adjust the number of retained messages for efficient memory management. The AI then asks if James is interested in how these memory types impact conversation quality or their technical implementations. James notes that buffer memory just stores the entire conversation. The AI agrees, explaining that ConversationBufferMemory stores the entire conversation history for context but can lead to challenges like increased memory usage. In contrast, ConversationBufferWindowMemory retains only a certain number of recent messages to optimize memory usage while still providing context. The AI then asks if James is considering the trade-offs between these two types of memory in his research or looking into their effects on user experience. James clarifies that buffer window memory stores the last k messages, dropping the rest. The AI confirms this, explaining that ConversationBufferWindowMemory retains only the last k messages to manage memory efficiently, allowing for dynamic interaction while minimizing memory usage. The AI then asks if James is exploring how this trade-off affects user experience or specific applications where one memory type might be preferred over the other.\n",
            "Human: What is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is my name again?',\n",
              " 'history': 'The human introduces himself as James, and the AI greets him, asking about his day. James mentions he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory includes short-term memory for single conversations, long-term memory for retaining information across interactions, contextual memory for maintaining coherence, and dynamic memory for adapting to new information. The AI then inquires if James is focusing on a specific type of conversational memory or exploring the topic more broadly. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI explains that ConversationBufferMemory tracks recent exchanges to maintain context, while ConversationBufferWindowMemory uses a sliding window to dynamically adjust the number of retained messages for efficient memory management. The AI then asks if James is interested in how these memory types impact conversation quality or their technical implementations. James notes that buffer memory just stores the entire conversation. The AI agrees, explaining that ConversationBufferMemory stores the entire conversation history for context but can lead to challenges like increased memory usage. In contrast, ConversationBufferWindowMemory retains only a certain number of recent messages to optimize memory usage while still providing context. The AI then asks if James is considering the trade-offs between these two types of memory in his research or looking into their effects on user experience. James clarifies that buffer window memory stores the last k messages, dropping the rest. The AI confirms this, explaining that ConversationBufferWindowMemory retains only the last k messages to manage memory efficiently, allowing for dynamic interaction while minimizing memory usage. The AI then asks if James is exploring how this trade-off affects user experience or specific applications where one memory type might be preferred over the other.',\n",
              " 'response': \"Your name is James! How's your research going?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"What is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZpfPyEztaTR"
      },
      "source": [
        "As this information was stored in the summary the LLM successfully recalled our name. This may not always be the case, by summarizing the conversation we inevitably compress the full amount of information and so we may lose key details occasionally. Nonetheless, this is a great memory type for long conversations while retaining some key information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snlKvJDUtaTR"
      },
      "source": [
        "### `ConversationSummaryMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9jdJCkztaTR"
      },
      "source": [
        "Let's implement this memory type using the `RunnableWithMessageHistory` class. As with the window buffer memory, we need to define a custom implementation of the `InMemoryChatMessageHistory` class. We'll call this one `ConversationSummaryMessageHistory`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "C09U1WkqtaTR"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "\n",
        "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI):\n",
        "        super().__init__(llm=llm)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages.\n",
        "        \"\"\"\n",
        "        self.messages.extend(messages)\n",
        "        # construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{messages}\"\n",
        "            )\n",
        "        ])\n",
        "        # format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=self.messages.content,\n",
        "                messages=[x.content for x in messages]\n",
        "            )\n",
        "        )\n",
        "        # replace the existing history with a single system summary message\n",
        "        self.messages = [SystemMessage(content=new_summary.content)]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "762Vg5u4taTR"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatOpenAI) -> ConversationSummaryMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "om78sKCGtaTS"
      },
      "outputs": [],
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatOpenAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPfut3S5taTS"
      },
      "source": [
        "Now we invoke our runnable, this time passing a `llm` parameter via the `config` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1F1m9n2taTS",
        "outputId": "bdddf98b-92f7-4b8e-bf55-3fbde647c6e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 26, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-69ff832c-7a98-4daf-a1e3-313f1fc9e1a6-0', usage_metadata={'input_tokens': 26, 'output_tokens': 11, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYqOCHCTtaTS"
      },
      "source": [
        "Let's see what summary was generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBDeudJftaTS",
        "outputId": "3cae0b7e-9fb3-4141-9e61-213afeb60e63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 26, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-69ff832c-7a98-4daf-a1e3-313f1fc9e1a6-0', usage_metadata={'input_tokens': 26, 'output_tokens': 11, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6uLeGXztaTS"
      },
      "source": [
        "Let's continue the conversation and see if the summary is updated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di7WVWmCtaTS",
        "outputId": "81dad28e-bf43-477d-937d-632dc26b6636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 26, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-69ff832c-7a98-4daf-a1e3-313f1fc9e1a6-0', usage_metadata={'input_tokens': 26, 'output_tokens': 11, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That sounds interesting! Conversational memory can refer to various concepts depending on the context, such as in human communication, artificial intelligence, or even in psychology. Here are a few types you might consider:\\n\\n1. **Short-term Memory**: This is the ability to hold a small amount of information in an active, readily available state for a brief period. In conversations, this allows individuals to remember what was just said to respond appropriately.\\n\\n2. **Long-term Memory**: This involves storing information over a longer period. In conversations, this can include remembering past interactions, personal details about the other person, or relevant information that can be brought up later.\\n\\n3. **Contextual Memory**: This type of memory involves remembering the context in which information was shared. It helps individuals recall not just the content of a conversation but also the circumstances surrounding it, such as the location or emotional tone.\\n\\n4. **Semantic Memory**: This refers to the memory of facts and general knowledge. In conversations, it allows individuals to draw on shared knowledge or cultural references to enhance communication.\\n\\n5. **Episodic Memory**: This is the ability to recall specific events or experiences. In conversations, it allows individuals to share personal stories or recall specific details from past interactions.\\n\\n6. **Social Memory**: This type of memory pertains to the knowledge of social norms, relationships, and the dynamics between individuals. It helps in navigating social interactions and understanding the roles of different participants in a conversation.\\n\\n7. **Conversational Memory in AI**: In the context of artificial intelligence, conversational memory refers to the ability of a system to remember past interactions with users. This can enhance user experience by providing personalized responses based on previous conversations.\\n\\nAre you looking for information on a specific type of conversational memory, or do you have a particular context in mind?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 369, 'prompt_tokens': 53, 'total_tokens': 422, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-2871da73-7ab5-41db-8e7c-2a7637ba21c9-0', usage_metadata={'input_tokens': 53, 'output_tokens': 369, 'total_tokens': 422, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")\n",
        "\n",
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqEv8kxZtaTS"
      },
      "source": [
        "So far so good! Let's continue with a few more messages before returning to the name question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "F7cy8dKZtaTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4164abf3-ddb7-495b-a8ef-db958f63b8fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n",
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n",
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        }
      ],
      "source": [
        "for msg in [\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnOdFy4htaTS"
      },
      "source": [
        "Let's see the latest summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SEBczVutaTS",
        "outputId": "84f3ae00-b1ca-4da3-fcc4-6dcc64206d55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 26, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-69ff832c-7a98-4daf-a1e3-313f1fc9e1a6-0', usage_metadata={'input_tokens': 26, 'output_tokens': 11, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That sounds interesting! Conversational memory can refer to various concepts depending on the context, such as in human communication, artificial intelligence, or even in psychology. Here are a few types you might consider:\\n\\n1. **Short-term Memory**: This is the ability to hold a small amount of information in an active, readily available state for a brief period. In conversations, this allows individuals to remember what was just said to respond appropriately.\\n\\n2. **Long-term Memory**: This involves storing information over a longer period. In conversations, this can include remembering past interactions, personal details about the other person, or relevant information that can be brought up later.\\n\\n3. **Contextual Memory**: This type of memory involves remembering the context in which information was shared. It helps individuals recall not just the content of a conversation but also the circumstances surrounding it, such as the location or emotional tone.\\n\\n4. **Semantic Memory**: This refers to the memory of facts and general knowledge. In conversations, it allows individuals to draw on shared knowledge or cultural references to enhance communication.\\n\\n5. **Episodic Memory**: This is the ability to recall specific events or experiences. In conversations, it allows individuals to share personal stories or recall specific details from past interactions.\\n\\n6. **Social Memory**: This type of memory pertains to the knowledge of social norms, relationships, and the dynamics between individuals. It helps in navigating social interactions and understanding the roles of different participants in a conversation.\\n\\n7. **Conversational Memory in AI**: In the context of artificial intelligence, conversational memory refers to the ability of a system to remember past interactions with users. This can enhance user experience by providing personalized responses based on previous conversations.\\n\\nAre you looking for information on a specific type of conversational memory, or do you have a particular context in mind?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 369, 'prompt_tokens': 53, 'total_tokens': 422, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-2871da73-7ab5-41db-8e7c-2a7637ba21c9-0', usage_metadata={'input_tokens': 53, 'output_tokens': 369, 'total_tokens': 422, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Great! It sounds like you're delving into specific implementations of conversational memory, particularly in the context of AI and natural language processing. Here’s a brief overview of both **ConversationBufferMemory** and **ConversationBufferWindowMemory**:\\n\\n### ConversationBufferMemory\\n\\n- **Definition**: This type of memory stores the entire conversation history in a buffer. It allows the AI to access all previous messages in the conversation, which can be useful for maintaining context and continuity.\\n  \\n- **Use Case**: It is particularly beneficial in scenarios where the context of the entire conversation is important for generating coherent and relevant responses. For example, in customer support chatbots, having access to the full conversation can help the bot provide better assistance.\\n\\n- **Limitations**: The main drawback is that as the conversation grows longer, the memory can become unwieldy, and it may lead to performance issues or increased computational costs. Additionally, it may not be efficient for very long conversations, as it could exceed memory limits.\\n\\n### ConversationBufferWindowMemory\\n\\n- **Definition**: This type of memory maintains a fixed-size window of the most recent messages in the conversation. Instead of storing the entire conversation, it only keeps a specified number of the latest exchanges.\\n\\n- **Use Case**: This approach is useful when the most recent context is more relevant than older messages. It can help in scenarios where conversations are dynamic and the context changes rapidly, such as in real-time chat applications.\\n\\n- **Limitations**: The main limitation is that it may lose important context from earlier in the conversation if it exceeds the fixed window size. This can lead to misunderstandings or less coherent responses if the AI needs information from earlier exchanges.\\n\\n### Comparison\\n\\n- **Memory Size**: ConversationBufferMemory can grow indefinitely (or until a limit is reached), while ConversationBufferWindowMemory has a fixed size.\\n  \\n- **Context Retention**: ConversationBufferMemory retains all context, while ConversationBufferWindowMemory focuses on the most recent context.\\n\\n- **Performance**: ConversationBufferWindowMemory is generally more efficient in terms of memory usage and processing speed, especially for long conversations.\\n\\n### Conclusion\\n\\nBoth types of memory have their advantages and disadvantages, and the choice between them often depends on the specific requirements of the application and the nature of the conversations being handled. If you have any specific questions or need further details about either type, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 483, 'prompt_tokens': 443, 'total_tokens': 926, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-00630377-545b-4815-b35d-a17d40a88145-0', usage_metadata={'input_tokens': 443, 'output_tokens': 483, 'total_tokens': 926, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Yes, that's correct! **ConversationBufferMemory** is designed to store the entire conversation history. This means that it keeps a record of all the messages exchanged between the user and the AI throughout the interaction. Here are some key points about it:\\n\\n### Key Features of ConversationBufferMemory\\n\\n1. **Complete History**: It retains every message from the start of the conversation until the end, allowing the AI to reference any part of the dialogue when generating responses.\\n\\n2. **Contextual Awareness**: By having access to the full conversation, the AI can maintain context more effectively, which is crucial for understanding nuances, references, and the flow of the dialogue.\\n\\n3. **Use Cases**: This type of memory is particularly useful in applications where continuity is important, such as:\\n   - Customer support chatbots that need to track user issues over time.\\n   - Personal assistants that remember user preferences and past interactions.\\n   - Any scenario where detailed context is necessary for coherent responses.\\n\\n4. **Limitations**: While it provides comprehensive context, there are some drawbacks:\\n   - **Memory Usage**: As the conversation length increases, the memory usage can grow significantly, which may lead to performance issues.\\n   - **Relevance**: In long conversations, older messages may become less relevant, and the AI might still need to focus on the most pertinent information.\\n\\n### Conclusion\\n\\nOverall, ConversationBufferMemory is a powerful tool for maintaining context in conversations, but it requires careful management to ensure that it operates efficiently, especially in longer interactions. If you have more specific questions about its implementation or use cases, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 327, 'prompt_tokens': 940, 'total_tokens': 1267, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-3aca609c-6a85-4c71-be23-f70f5bca1614-0', usage_metadata={'input_tokens': 940, 'output_tokens': 327, 'total_tokens': 1267, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Yes, that's correct! **ConversationBufferWindowMemory** is designed to store only the most recent \\\\( k \\\\) messages in a conversation, effectively creating a sliding window of context. Here are some key points about this type of memory:\\n\\n### Key Features of ConversationBufferWindowMemory\\n\\n1. **Fixed Size**: It maintains a fixed-size buffer that holds the last \\\\( k \\\\) messages exchanged in the conversation. Once the limit is reached, the oldest messages are dropped to make room for new ones.\\n\\n2. **Recent Context**: This approach allows the AI to focus on the most recent exchanges, which are often the most relevant for generating appropriate responses. It is particularly useful in dynamic conversations where context can change rapidly.\\n\\n3. **Use Cases**: ConversationBufferWindowMemory is beneficial in scenarios such as:\\n   - Real-time chat applications where quick responses are needed.\\n   - Situations where the conversation context is likely to evolve quickly, making older messages less relevant.\\n   - Interactive applications where users may frequently change topics.\\n\\n4. **Efficiency**: By limiting the amount of stored data, this type of memory can be more efficient in terms of memory usage and processing speed compared to storing the entire conversation history.\\n\\n5. **Limitations**: The main drawback is that it may lose important context from earlier in the conversation if those messages are outside the fixed window. This can lead to misunderstandings or less coherent responses if the AI needs information that has been dropped.\\n\\n### Conclusion\\n\\nIn summary, ConversationBufferWindowMemory is a practical approach for managing conversational context in a way that balances relevance and efficiency. It allows for quick access to the most pertinent information while avoiding the pitfalls of excessive memory usage. If you have any further questions or need more details about its implementation or applications, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 363, 'prompt_tokens': 1287, 'total_tokens': 1650, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-0c444735-eb50-468c-a8ab-40465e82d422-0', usage_metadata={'input_tokens': 1287, 'output_tokens': 363, 'total_tokens': 1650, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcR46iNxtaTS"
      },
      "source": [
        "The information about our name has been maintained, so let's see if this is enough for our LLM to correctly recall our name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doVtbwVytaTS",
        "outputId": "e51ac466-d0a1-4d46-d7ed-dd657d277b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James! How can I assist you further today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 1663, 'total_tokens': 1677, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-d2296651-4454-496b-a918-327c82797c89-0', usage_metadata={'input_tokens': 1663, 'output_tokens': 14, 'total_tokens': 1677, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2A8VfZctaTS"
      },
      "source": [
        "Perfect! We've successfully implemented the `ConversationSummaryMemory` type using the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpJM9TsltaTS"
      },
      "source": [
        "## 4. `ConversationSummaryBufferMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmF85VmAtaTS"
      },
      "source": [
        "Our final memory type acts as a combination of `ConversationSummaryMemory` and `ConversationBufferMemory`. It keeps the buffer for the conversation up until the previous `n` tokens, anything beyond that limit is summarized then dropped from the buffer. Producing something like:\n",
        "\n",
        "\n",
        "```\n",
        "# ~~ a summary of previous interactions\n",
        "The user named James introduced himself and the AI responded, introducing itself as an AI model called Zeta.\n",
        "James then said he was researching the different types of conversational memory and Zeta asked for some\n",
        "examples.\n",
        "# ~~ the most recent messages\n",
        "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
        "AI: That's interesting, what's the difference?\n",
        "Human: Buffer memory just stores the entire conversation\n",
        "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
        "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
        "AI: Very cool!\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tPilUOHtaTT",
        "outputId": "83a66c69-e069-4cab-8fac-621e3c3d24b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-ed1cccbb366f>:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(\n",
        "    llm=llm,\n",
        "    max_token_limit=300,\n",
        "    return_messages=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvUBQXmBtaTT"
      },
      "source": [
        "As before, we set up the deprecated memory type using the `ConversationChain` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "HdcdtYdrtaTT"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsKcrFxOtaTT"
      },
      "source": [
        "First invoke with a single message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAZqJKHGtaTT",
        "outputId": "a889a715-91fd-4e5a-9e7b-f1244ccd7af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: Hi, my name is James\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Hi, my name is James',\n",
              " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hello, James! It's great to meet you! I'm here to chat and help with anything you might need. What’s on your mind today?\", additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"Hello, James! It's great to meet you! I'm here to chat and help with anything you might need. What’s on your mind today?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"Hi, my name is James\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma-CwiwztaTT"
      },
      "source": [
        "Looks good so far, let's continue with a few more messages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlsR1iaItaTT",
        "outputId": "fea78040-2bf2-4212-e6d4-82253f3df1bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello, James! It's great to meet you! I'm here to chat and help with anything you might need. What’s on your mind today?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: I'm researching the different types of conversational memory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='The human, named James, introduces himself and expresses interest in researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory allows systems to remember information from previous interactions for a personalized experience. It outlines five types of conversational memory: short-term memory (context within a single conversation), long-term memory (retaining information across multiple interactions), contextual memory (understanding conversation flow), dynamic memory (updating memory based on new information), and episodic memory (recalling specific events or interactions). The AI notes the advantages and challenges of each type, particularly concerning privacy and data management, and asks if James is looking into specific applications or systems that utilize these memory types.', additional_kwargs={}, response_metadata={})]\n",
            "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='The human, named James, introduces himself and expresses interest in researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory allows systems to remember information from previous interactions for a personalized experience. It outlines five types of conversational memory: short-term memory (context within a single conversation), long-term memory (retaining information across multiple interactions), contextual memory (understanding conversation flow), dynamic memory (updating memory based on new information), and episodic memory (recalling specific events or interactions). The AI notes the advantages and challenges of each type, particularly concerning privacy and data management, and asks if James is looking into specific applications or systems that utilize these memory types.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's great! Both ConversationBufferMemory and ConversationBufferWindowMemory are interesting approaches to managing conversational memory. \\n\\n**ConversationBufferMemory** typically retains the entire conversation history in a buffer, allowing the AI to reference previous exchanges during the current interaction. This can be particularly useful for maintaining context and continuity, but it can also lead to challenges with memory management, especially if the conversation becomes lengthy. The AI can pull relevant information from the entire history, which helps in providing more coherent and contextually aware responses.\\n\\nOn the other hand, **ConversationBufferWindowMemory** operates on a more constrained model. It maintains a fixed-size window of the most recent interactions, which means that as new messages come in, older ones are discarded. This approach can help manage memory usage more effectively and keep the conversation focused on the most relevant context. However, it may lose some important details from earlier in the conversation if they fall outside the window.\\n\\nAre you considering using these memory types for a specific project or application?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: Buffer memory just stores the entire conversation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='The human, named James, introduces himself and expresses interest in researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory allows systems to remember information from previous interactions for a personalized experience. It outlines five types of conversational memory: short-term memory, long-term memory, contextual memory, dynamic memory, and episodic memory, noting their advantages and challenges, particularly regarding privacy and data management. James mentions he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory, to which the AI responds that both are interesting approaches. It explains that ConversationBufferMemory retains the entire conversation history for context but can face memory management challenges, while ConversationBufferWindowMemory maintains a fixed-size window of recent interactions to manage memory usage effectively, though it may lose important details. The AI then asks if James is considering using these memory types for a specific project or application.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Yes, that's correct! Buffer memory, like ConversationBufferMemory, stores the entire conversation history, allowing the AI to access all previous exchanges during the current interaction. This can be beneficial for maintaining context and ensuring that responses are coherent and relevant to the ongoing discussion. However, it can also lead to challenges, such as increased memory usage and potential difficulties in managing large amounts of data, especially in lengthy conversations. \\n\\nIf you're exploring this for a specific project, it might be helpful to consider how much context you need to retain and whether the benefits of having the entire history outweigh the potential drawbacks. Are you thinking about implementing this in a particular application or use case?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "for msg in [\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    chain.invoke({\"input\": msg})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Rj5SMftaTT"
      },
      "source": [
        "We can see with each new message the initial `SystemMessage` is updated with a new summary of the conversation. This initial `SystemMessage` is then followed by the most recent `AIMessage` and `HumanMessage` objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Dnn_N8taTT"
      },
      "source": [
        "### `ConversationSummaryBufferMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72St8qDjtaTT"
      },
      "source": [
        "As with the previous memory types, we will implement this memory type again using the `RunnableWithMessageHistory` class. In our implementation we will modify the buffer window to be based on the number of messages rather than number of tokens. This tweak will make our implementation more closely aligned with original buffer window.\n",
        "\n",
        "We will implement all of this via a new `ConversationSummaryBufferMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "tWOvrR4wtaTT"
      },
      "outputs": [],
      "source": [
        "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI, k: int):\n",
        "        super().__init__(llm=llm, k=k)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages and summarizing the messages that we\n",
        "        drop.\n",
        "        \"\"\"\n",
        "        existing_summary: SystemMessage | None = None\n",
        "        old_messages: list[BaseMessage] | None = None\n",
        "        # see if we already have a summary message\n",
        "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
        "            print(\">> Found existing summary\")\n",
        "            existing_summary = self.messages.pop(0)\n",
        "        # add the new messages to the history\n",
        "        self.messages.extend(messages)\n",
        "        # check if we have too many messages\n",
        "        if len(self.messages) > self.k:\n",
        "            print(\n",
        "                f\">> Found {len(self.messages)} messages, dropping \"\n",
        "                f\"oldest {len(self.messages) - self.k} messages.\")\n",
        "            # pull out the oldest messages...\n",
        "            old_messages = self.messages[:self.k]\n",
        "            # ...and keep only the most recent messages\n",
        "            self.messages = self.messages[-self.k:]\n",
        "        if old_messages is None:\n",
        "            print(\">> No old messages to update summary with\")\n",
        "            # if we have no old_messages, we have nothing to update in summary\n",
        "            return\n",
        "        # construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{old_messages}\"\n",
        "            )\n",
        "        ])\n",
        "        # format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=existing_summary,\n",
        "                old_messages=old_messages\n",
        "            )\n",
        "        )\n",
        "        print(f\">> New summary: {new_summary.content}\")\n",
        "        # prepend the new summary to the history\n",
        "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ITlLoeAtaTT"
      },
      "source": [
        "Redefine the `get_chat_history` function to use our new `ConversationSummaryBufferMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "rvWsP1GMtaTT"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatOpenAI, k: int) -> ConversationSummaryBufferMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HbocfKdtaTT"
      },
      "source": [
        "Setup our pipeline with new configurable fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "cQ_-21_3taTU"
      },
      "outputs": [],
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatOpenAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10GMrRXutaTU"
      },
      "source": [
        "Finally, we invoke our runnable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWfJn7DKtaTU",
        "outputId": "e985271d-b12f-4c0a-d52e-993d19b9c36d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> No old messages to update summary with\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 26, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-f44aca3f-31b2-4b4c-88ad-61313f64e24d-0', usage_metadata={'input_tokens': 26, 'output_tokens': 11, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        ")\n",
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mODJviTtaTU",
        "outputId": "c8fd8e52-7db1-49eb-f8e0-40f69d9f5c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "Message 1\n",
            "---\n",
            "\n",
            ">> No old messages to update summary with\n",
            "---\n",
            "Message 2\n",
            "---\n",
            "\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: In the conversation, James introduces himself and expresses interest in researching different types of conversational memory. The AI responds positively, providing a detailed overview of various types of conversational memory, including:\n",
            "\n",
            "1. **Short-term Memory**: Retaining information for a brief period to respond appropriately in conversations.\n",
            "2. **Long-term Memory**: Storing information over a longer duration, including past interactions and personal details.\n",
            "3. **Contextual Memory**: Remembering the context of shared information to understand conversation nuances.\n",
            "4. **Semantic Memory**: Knowledge of facts and concepts not tied to personal experiences.\n",
            "5. **Episodic Memory**: Recalling specific events or experiences from one’s life.\n",
            "6. **Conversational Memory in AI**: The ability of AI systems to remember past interactions to enhance user experience.\n",
            "7. **Social Memory**: The collective memory of a group that influences shared narratives.\n",
            "\n",
            "The AI concludes by asking if James is looking for information on a specific type of conversational memory or has a particular context in mind.\n",
            "---\n",
            "Message 3\n",
            "---\n",
            "\n",
            ">> Found existing summary\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: In the conversation, James expresses his interest in researching different types of conversational memory. The AI responds with an overview of various types of conversational memory, including:\n",
            "\n",
            "1. **Short-term Memory**: Retaining information briefly for appropriate responses.\n",
            "2. **Long-term Memory**: Storing information over longer periods, including past interactions.\n",
            "3. **Contextual Memory**: Remembering the context of shared information to understand nuances.\n",
            "4. **Semantic Memory**: Knowledge of facts and concepts not tied to personal experiences.\n",
            "5. **Episodic Memory**: Recalling specific events or experiences from one’s life.\n",
            "6. **Conversational Memory in AI**: The ability of AI systems to remember past interactions to enhance user experience.\n",
            "7. **Social Memory**: The collective memory of a group influencing shared narratives.\n",
            "\n",
            "James then mentions he has been looking at **ConversationBufferMemory** and **ConversationBufferWindowMemory**. The AI provides detailed explanations of both concepts:\n",
            "\n",
            "- **ConversationBufferMemory**: Stores the entire conversation history, allowing the AI to maintain context but can become unwieldy with longer conversations.\n",
            "- **ConversationBufferWindowMemory**: Maintains a fixed-size window of recent messages, improving performance but potentially losing important earlier context.\n",
            "\n",
            "The AI compares the two types, noting their differences in memory size, context retention, and performance efficiency. It invites James to ask further questions if he needs more details.\n",
            "---\n",
            "Message 4\n",
            "---\n",
            "\n",
            ">> Found existing summary\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: In the conversation, James expresses his interest in researching different types of conversational memory, specifically mentioning **ConversationBufferMemory** and **ConversationBufferWindowMemory**. The AI provides a detailed overview of both concepts:\n",
            "\n",
            "1. **ConversationBufferMemory**:\n",
            "   - **Definition**: Stores the entire conversation history, allowing the AI to access all previous messages.\n",
            "   - **Use Case**: Useful in applications where maintaining complete context is important, such as customer support or personal assistants.\n",
            "   - **Limitations**: Can become unwieldy and resource-intensive as conversations grow longer, potentially leading to performance issues.\n",
            "\n",
            "2. **ConversationBufferWindowMemory**:\n",
            "   - **Definition**: Maintains a fixed-size window of recent messages, rather than the entire conversation.\n",
            "   - **Use Case**: Ideal for scenarios where only the most recent context is relevant, improving performance and managing memory usage.\n",
            "   - **Limitations**: May lose important earlier context if the conversation exceeds the window size, which can affect response coherence.\n",
            "\n",
            "The AI compares the two types, highlighting differences in memory size, context retention, and performance efficiency. James confirms his understanding that buffer memory stores the entire conversation, and the AI elaborates on the key features of **ConversationBufferMemory**, emphasizing its complete history retention, contextual awareness, and resource considerations. The AI notes that while it is powerful for maintaining conversational context, it requires careful management to ensure efficiency. James is invited to ask further questions if needed.\n"
          ]
        }
      ],
      "source": [
        "for i, msg in enumerate([\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]):\n",
        "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU_Xx40HtaTU"
      },
      "source": [
        "There we go, we've successfully implemented the `ConversationSummaryBufferMemory` type using `RunnableWithMessageHistory`!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}